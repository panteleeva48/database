{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pprint import pprint\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb+srv://impanteleyeva:Vasya123@cluster0-9jzor.mongodb.net/test?retryWrites=true',\n",
    "                             ssl=True,\n",
    "                             ssl_cert_reqs=ssl.CERT_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.drop_database('realec_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.realec_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = db.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем файлы, у которых есть оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = './exam'\n",
    "os.remove('files_with_json.txt')\n",
    "with open('files_with_json.txt', 'a') as fw:\n",
    "    for root, dirs, files in os.walk(main_dir):\n",
    "        for name in files:\n",
    "            if '.json' in name:\n",
    "                full_path = os.path.join(root, name)\n",
    "                try:\n",
    "                    with open(full_path) as fj:\n",
    "                        data = json.load(fj)\n",
    "                        if 'mark' in data.keys() and data['mark'] != '':\n",
    "                            fw.write(os.path.join(root, name)[:-5] + '\\n')\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таких файлов получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6586\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l < files_with_json.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mark(mark):\n",
    "    mark = str(mark)\n",
    "    mark = mark.replace('%', '').strip()\n",
    "    mark = mark.replace('.', '').strip()\n",
    "    mark = mark.replace(',', '').strip()\n",
    "    mark = mark.replace('(overall)', '').strip()\n",
    "    if '/' in mark:\n",
    "        exp = re.search('([0-9]{2,3})/([0-9]{2,3})', mark)\n",
    "        first = exp.group(1)\n",
    "        second = exp.group(2)\n",
    "        mark = (int(first) + int(second)) / 2\n",
    "    elif 'не дописан автором' in mark:\n",
    "        mark = re.search('([0-9]{2,3})', mark).group(1)\n",
    "    elif '(' in mark and ')' in mark:\n",
    "        exp = re.search('([0-9]{2,3}) *\\(([0-9]{2,3}).*\\)', mark)\n",
    "        first = exp.group(1)\n",
    "        second = exp.group(2)\n",
    "        mark = (int(first) + int(second)) / 2\n",
    "    return mark   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "\n",
    "open_class = [\"NOUN\", \"VERB\", \"ADV\", \"ADJ\"]\n",
    "with open('lists.json') as data_file:\n",
    "    lists = json.load(data_file)\n",
    "fivetfrequentCOCA = lists['5000frequentCOCA']\n",
    "frequentverbsCOCAfromfivet = lists['frequentverbsCOCAfrom5000']\n",
    "uwl = lists['UWL']\n",
    "\n",
    "class LexicalComplexity:\n",
    "    \"\"\"Returns values of lexical criteria.\"\"\"\n",
    "    \n",
    "    def __init__(self, text, parser):\n",
    "        self.text = text\n",
    "        self.parser = parser\n",
    "    \n",
    "    def get_verb_lemmas(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'VERB']['Lemma']\n",
    "    \n",
    "    def get_noun_lemmas(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'NOUN']['Lemma']\n",
    "\n",
    "    def get_adj_lemmas(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADJ']['Lemma']\n",
    "    \n",
    "    def get_adv_lemmas(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df[df['UPosTag'] == 'ADV']['Lemma']\n",
    "    \n",
    "    def get_lex_lemmas(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df[df['UPosTag'].isin(open_class)]['Lemma']\n",
    "    \n",
    "    def get_lemmas(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df['Lemma']\n",
    "    \n",
    "    def get_forms(self):\n",
    "        df = self.parser.conllu2df()\n",
    "        return df['Form']\n",
    "    \n",
    "    def safe_divide(self, numerator, denominator):\n",
    "        if denominator == 0 or denominator == 0.0:\n",
    "            index = 0\n",
    "        else: index = numerator/denominator\n",
    "        return index\n",
    "\n",
    "    def division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def corrected_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(2*len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def root_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)/math.sqrt(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def squared_division(self, list1, list2):\n",
    "        try:\n",
    "            return len(list1)**2/len(list2)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def log_division(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))/math.log(len(list2))\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def uber(self, list1, list2):\n",
    "        try:\n",
    "            return math.log(len(list1))**2/math.log(len(set(list2))/len(list1))\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def density(self, punct=False):\n",
    "        \"\"\"\n",
    "        number of lexical tokens/number of tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        lemmas = self.get_lemmas()\n",
    "        return self.division(lex_lemmas, lemmas)\n",
    "    \n",
    "    def LS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated lexical tokens/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = self.get_lex_lemmas()\n",
    "        soph_lex_lemmas = [i for i in lex_lemmas if i not in fivetfrequentCOCA]\n",
    "        return self.division(soph_lex_lemmas, lex_lemmas)\n",
    "    \n",
    "    def VS(self):\n",
    "        \"\"\"\n",
    "        number of sophisticated verb lemmas/number of verb tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = self.get_verb_lemmas()\n",
    "        soph_verbs = set([i for i in verb_lemmas if i not in frequentverbsCOCAfromfivet])\n",
    "        VSI = self.division(soph_verbs, verb_lemmas)\n",
    "        VSII = self.corrected_division(soph_verbs, verb_lemmas)\n",
    "        VSIII = self.squared_division(soph_verbs, verb_lemmas)\n",
    "        return VSI, VSII, VSIII\n",
    "\n",
    "    def LFP(self):\n",
    "        \"\"\"\n",
    "        Lexical Frequency Profile is the proportion of tokens:\n",
    "        first - 1000 most frequent words\n",
    "        second list - the second 1000\n",
    "        third - University Word List (Xue & Nation 1989)\n",
    "        none - list of those that are not in these lists\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        first = [i for i in lemmas if i in fivetfrequentCOCA[0:1000]]\n",
    "        second = [i for i in lemmas if i in fivetfrequentCOCA[1000:2000]]\n",
    "        third = [i for i in lemmas if i in uwl]\n",
    "        first_procent = self.division(first, lemmas)\n",
    "        second_procent = self.division(second, lemmas)\n",
    "        third_procent = self.division(third, lemmas)\n",
    "        none = 1 - (first_procent + second_procent + third_procent)\n",
    "        return first_procent, second_procent , third_procent, none\n",
    "    \n",
    "    def NDW(self):\n",
    "        \"\"\"\n",
    "        number of lemmas\n",
    "        \"\"\"\n",
    "        lemmas = self.get_lemmas()\n",
    "        return len(set(lemmas))\n",
    "    \n",
    "    def TTR(self):\n",
    "        \"\"\"\n",
    "        number of lemmas/number of tokens\n",
    "        \"\"\"\n",
    "        lemmas = set(self.get_lemmas())\n",
    "        tokens = self.get_lemmas()\n",
    "        TTR = self.division(lemmas, tokens)\n",
    "        CTTR = self.corrected_division(lemmas, tokens)\n",
    "        RTTR = self.root_division(lemmas, tokens)\n",
    "        LogTTR = self.log_division(lemmas, tokens)\n",
    "        Uber = self.uber(lemmas, tokens)\n",
    "        return TTR, CTTR, RTTR, LogTTR, Uber\n",
    "\n",
    "    def choose(self, n, k):\n",
    "        \"\"\"\n",
    "        Calculates binomial coefficients\n",
    "        \"\"\"\n",
    "        if 0 <= k <= n:\n",
    "            ntok = 1\n",
    "            ktok = 1\n",
    "            for t in range(1, min(k, n - k) + 1):\n",
    "                ntok *= n\n",
    "                ktok *= t\n",
    "                n -= 1\n",
    "            return ntok // ktok\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def hyper(self, successes, sample_size, population_size, freq):\n",
    "        \"\"\"\n",
    "        Calculates hypergeometric distribution\n",
    "        \"\"\"\n",
    "        # probability a word will occur at least once in a sample of a particular size\n",
    "        try:\n",
    "            prob_1 = 1.0 - (float((self.choose(freq, successes) * \n",
    "                                   self.choose((population_size - freq),\n",
    "                                               (sample_size - successes)))) /\n",
    "                            float(self.choose(population_size, sample_size)))\n",
    "            prob_1 = prob_1 * (1/sample_size)\n",
    "        except ZeroDivisionError:\n",
    "            prob_1 = 0\n",
    "        return prob_1\n",
    "    \n",
    "    def D(self):\n",
    "        prob_sum = 0.0\n",
    "        tokens = self.get_forms()\n",
    "        num_tokens = len(tokens)\n",
    "        types_list = list(set(tokens))\n",
    "        frequency_dict = collections.Counter(tokens)\n",
    "\n",
    "        for items in types_list:\n",
    "            # random sample is 42 items in length\n",
    "            prob = self.hyper(0, 42, num_tokens, frequency_dict[items])\n",
    "            prob_sum += prob\n",
    "\n",
    "        return prob_sum\n",
    "\n",
    "    def LV(self):\n",
    "        \"\"\"\n",
    "        number of lexical lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        lex_lemmas = set(self.get_lex_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return len(lex_lemmas)/len(lex_tokens)\n",
    "    \n",
    "    def VV(self):\n",
    "        \"\"\"\n",
    "        VVI: number of verb lemmas/number of verb tokens\n",
    "        VVII: number of verb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        verb_lemmas = set(self.get_verb_lemmas())\n",
    "        verb_tokens = self.get_verb_lemmas()\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        VVI = self.division(verb_lemmas, verb_tokens)\n",
    "        SVVI = self.squared_division(verb_lemmas, verb_tokens)\n",
    "        CVVI = self.corrected_division(verb_lemmas, verb_tokens)\n",
    "        VVII = self.division(verb_lemmas, lex_tokens)\n",
    "        return VVI, SVVI, CVVI, VVII\n",
    "        \n",
    "    def NV(self):\n",
    "        \"\"\"\n",
    "        number of noun lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        noun_lemmas = set(self.get_noun_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(noun_lemmas, lex_tokens)\n",
    "\n",
    "    def AdjV(self):\n",
    "        \"\"\"\n",
    "        number of adjective lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adj_lemmas = set(self.get_adj_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(adj_lemmas, lex_tokens)\n",
    "    \n",
    "    def AdvV(self):\n",
    "        \"\"\"\n",
    "        number of adverb lemmas/number of lexical tokens\n",
    "        \"\"\"\n",
    "        adv_lemmas = set(self.get_adv_lemmas())\n",
    "        lex_tokens = self.get_lex_lemmas()\n",
    "        return self.division(adv_lemmas, lex_tokens)\n",
    "    \n",
    "    def ModV(self):\n",
    "        return self.AdjV() + self.AdvV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import enchant.checker\n",
    "from enchant.checker.CmdLineChecker import CmdLineChecker\n",
    "def check_spelling(text):\n",
    "    chkr = enchant.checker.SpellChecker(\"en_GB\")\n",
    "    chkr.set_text(text)\n",
    "    for err in chkr:\n",
    "        sug = err.suggest()[0]\n",
    "        err.replace(sug)\n",
    "    c = chkr.get_text()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import ParserUDpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path):\n",
    "    with open(path + '.txt', 'r') as fr:\n",
    "        text = fr.read()\n",
    "    with open(path + '.json') as fj:\n",
    "        data = json.load(fj)\n",
    "        mark = data['mark']\n",
    "        mark = clean_mark(mark)\n",
    "        try:\n",
    "            mark = int(mark)\n",
    "        except:\n",
    "            print(path)\n",
    "            print('Field \"mark\" has something strange:', mark)\n",
    "    try:\n",
    "        _text = check_spelling(text)\n",
    "        _text = _text.replace('\\n', ' ')\n",
    "        parser = ParserUDpipe(_text)\n",
    "        LC = LexicalComplexity(_text, parser)\n",
    "        info = {'text': text, 'mark': mark, 'density': LC.density(), 'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "                         'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "                         'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "                         'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "                         'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.D(), \n",
    "                         'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "                         'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "                         'ModV': LC.ModV()}\n",
    "        return info\n",
    "    except:\n",
    "        print('Smth wrong with file', path)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0b81f820604b2883f0474436bec8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smth wrong with file ./exam/exam2014/AAl_9_2\n",
      "Smth wrong with file ./exam/exam2014/AMe_18_2\n",
      "Smth wrong with file ./exam/exam2014/AMe_30_1\n"
     ]
    }
   ],
   "source": [
    "with open('files_with_json.txt', 'r') as fr:\n",
    "    for line, path in tqdm(enumerate(fr)):\n",
    "        if line < 1000:\n",
    "            path = path[:-1]\n",
    "            info = get_info(path)\n",
    "            if info:\n",
    "                info['path'] = str(path)\n",
    "                text_id=texts.insert_one(info).inserted_id\n",
    "                del info\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "оказались пустыми файлами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdjV': 0.13513513513513514,\n",
      " 'AdvV': 0.10810810810810811,\n",
      " 'CTTR': 6.18666668897694,\n",
      " 'CVVI': 3.396831102433787,\n",
      " 'D': 0.8568365126220354,\n",
      " 'LFP_first': 0.7670807453416149,\n",
      " 'LFP_none': 0.14596273291925477,\n",
      " 'LFP_second': 0.06521739130434782,\n",
      " 'LFP_third': 0.021739130434782608,\n",
      " 'LS': 0.0945945945945946,\n",
      " 'LV': 0.722972972972973,\n",
      " 'LogTTR': 0.8756083940838074,\n",
      " 'ModV': 0.24324324324324326,\n",
      " 'NDW': 157,\n",
      " 'NV': 0.31756756756756754,\n",
      " 'RTTR': 8.74926793743304,\n",
      " 'SVVI': 23.076923076923077,\n",
      " 'TTR': 0.48757763975155277,\n",
      " 'Uber': 0,\n",
      " 'VSI': 0.05128205128205128,\n",
      " 'VSII': 0.22645540682891913,\n",
      " 'VSIII': 0.10256410256410256,\n",
      " 'VVI': 0.7692307692307693,\n",
      " 'VVII': 0.20270270270270271,\n",
      " '_id': ObjectId('5c83f4162964392ea9b5f237'),\n",
      " 'density': 0.45962732919254656,\n",
      " 'mark': 45,\n",
      " 'path': './exam/DTi_50_2',\n",
      " 'text': 'It is widely known that music labels and film makers lose a great '\n",
      "         'deal of money every year from illegal copying and free internet '\n",
      "         'sharing. Some people say that people who do that should be punished, '\n",
      "         'some think that such men are the new «Robin Hoods». Let us take a '\n",
      "         'look at this problem.\\n'\n",
      "         'On the one hand, illegal copying is prohibited in almost all '\n",
      "         'civilized countries and there is a reason for that. Music, books and '\n",
      "         'films are considered as an intellectual property and it is quite '\n",
      "         'uderstandable because artists are getting paid for composing, '\n",
      "         'painting and film making only if their products sell, and if they do '\n",
      "         'not have enough money for their living and creating they will just '\n",
      "         'get another job, and this is why the law of intellectual property '\n",
      "         'exists. It helps artists to get money they deserve and to have '\n",
      "         'enough funds to make a quality product.\\n'\n",
      "         'On the other hand, nowadays it is not so simple as it may look like '\n",
      "         'at first. Musicians do not often need labels to record their '\n",
      "         'masterpieces anymore because personal computers went so far that now '\n",
      "         'you can record some high-quality sound right in your living room so '\n",
      "         'you do not need to rent a studio for that. As for film makers, they '\n",
      "         'get millions just by dressing the main character in a big logo '\n",
      "         'T-shirt of some reach corporation. In fact, money, that a film '\n",
      "         'company is paid for commercial, can sometimes fully cover the film '\n",
      "         'making expences. \\n'\n",
      "         'To conclude, I would like to say that we should obey the law of '\n",
      "         'intellectual property. If we want to live in a respectful society, '\n",
      "         'but sometimes, I think we should also ask ourselves what we are '\n",
      "         'paying for.\\n'}\n"
     ]
    }
   ],
   "source": [
    "pprint(texts.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посчитаем среднее для всех групп по критериям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(val, my_dict):\n",
    "    for key, value in my_dict.items():\n",
    "        if val == value:\n",
    "            return key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(criterion, function):\n",
    "    # worst   [0:50]\n",
    "    # average (50:75)\n",
    "    # best    [75:100]\n",
    "    pipeline = [\n",
    "        {'$group': {\n",
    "            '_id': {\n",
    "                'worst': {\n",
    "                    '$%s' %function: {\n",
    "                        '$cond': [{'$lt': ['$mark', 51]}, 1, 0]\n",
    "                    }\n",
    "                },\n",
    "                'average': {\n",
    "                    '$%s' %function: {\n",
    "                        '$cond': [{'$and': [{'$gt': ['$mark', 50 ]}, {'$lt': ['$mark', 75 ]}]} , 1, 0]\n",
    "                    }\n",
    "                },\n",
    "                'best': {\n",
    "                    '$%s' %function: {\n",
    "                        '$cond': [{'$gt': ['$mark', 74]}, 1, 0]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            '%s' %function: {\n",
    "                '$%s' %function: '$%s' %criterion\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    ]\n",
    "    data = {}\n",
    "    for doc in texts.aggregate(pipeline):\n",
    "        group = get_key(1, doc['_id'])\n",
    "        data[group] = round(doc[function], 4)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = ['density', 'LS', 'VSI', 'VSII', 'VSIII', 'LFP_first',\n",
    "            'LFP_second', 'LFP_third', 'LFP_none', 'NDW', 'TTR',\n",
    "            'CTTR', 'RTTR', 'LogTTR', 'Uber', 'D', 'LV', 'VVI',\n",
    "            'SVVI', 'CVVI', 'VVII', 'NV', 'AdjV', 'AdvV', 'ModV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'best': [], 'worst': [], 'average': []}\n",
    "for c in criteria:\n",
    "    d = count(c, 'avg')\n",
    "    data['best'].append(d['best'])\n",
    "    data['worst'].append(d['worst'])\n",
    "    data['average'].append(d['average'])\n",
    "data['criterion'] = criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>worst</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>criterion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.4291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS</th>\n",
       "      <td>0.1357</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VSI</th>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0826</td>\n",
       "      <td>0.0713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VSII</th>\n",
       "      <td>0.3255</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>0.2312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VSIII</th>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.1882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFP_first</th>\n",
       "      <td>0.6555</td>\n",
       "      <td>0.6522</td>\n",
       "      <td>0.6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFP_second</th>\n",
       "      <td>0.0934</td>\n",
       "      <td>0.0930</td>\n",
       "      <td>0.0955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFP_third</th>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFP_none</th>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>0.2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDW</th>\n",
       "      <td>124.7000</td>\n",
       "      <td>89.5985</td>\n",
       "      <td>112.5497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTR</th>\n",
       "      <td>0.4741</td>\n",
       "      <td>0.4760</td>\n",
       "      <td>0.4437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTTR</th>\n",
       "      <td>5.3976</td>\n",
       "      <td>4.5025</td>\n",
       "      <td>4.9528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTTR</th>\n",
       "      <td>7.6334</td>\n",
       "      <td>6.3675</td>\n",
       "      <td>7.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogTTR</th>\n",
       "      <td>0.8640</td>\n",
       "      <td>0.8552</td>\n",
       "      <td>0.8513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uber</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.8053</td>\n",
       "      <td>0.7666</td>\n",
       "      <td>0.7907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LV</th>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>0.6486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VVI</th>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.7459</td>\n",
       "      <td>0.7422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVVI</th>\n",
       "      <td>15.8940</td>\n",
       "      <td>10.2769</td>\n",
       "      <td>12.8463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CVVI</th>\n",
       "      <td>2.7455</td>\n",
       "      <td>2.1744</td>\n",
       "      <td>2.4565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VVII</th>\n",
       "      <td>0.1616</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.3135</td>\n",
       "      <td>0.2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdjV</th>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.1329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdvV</th>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ModV</th>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.2229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                best    worst   average\n",
       "criterion                              \n",
       "density       0.4510   0.4375    0.4291\n",
       "LS            0.1357   0.1622    0.1295\n",
       "VSI           0.0958   0.0826    0.0713\n",
       "VSII          0.3255   0.2384    0.2312\n",
       "VSIII         0.3403   0.2175    0.1882\n",
       "LFP_first     0.6555   0.6522    0.6757\n",
       "LFP_second    0.0934   0.0930    0.0955\n",
       "LFP_third     0.0302   0.0248    0.0279\n",
       "LFP_none      0.2209   0.2299    0.2009\n",
       "NDW         124.7000  89.5985  112.5497\n",
       "TTR           0.4741   0.4760    0.4437\n",
       "CTTR          5.3976   4.5025    4.9528\n",
       "RTTR          7.6334   6.3675    7.0043\n",
       "LogTTR        0.8640   0.8552    0.8513\n",
       "Uber          0.0000   0.0000    0.0000\n",
       "D             0.8053   0.7666    0.7907\n",
       "LV            0.6919   0.6578    0.6486\n",
       "VVI           0.7974   0.7459    0.7422\n",
       "SVVI         15.8940  10.2769   12.8463\n",
       "CVVI          2.7455   2.1744    2.4565\n",
       "VVII          0.1616   0.1563    0.1527\n",
       "NV            0.3062   0.3135    0.2951\n",
       "AdjV          0.1467   0.1343    0.1329\n",
       "AdvV          0.0983   0.0770    0.0900\n",
       "ModV          0.2450   0.2113    0.2229"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data)\n",
    "df.set_index('criterion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поймём насколько статистически значимая разница между worst и best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистически значимая разница по критерию 'NDW'\n",
      "Статистически значимая разница по критерию 'CTTR'\n",
      "Статистически значимая разница по критерию 'RTTR'\n",
      "Статистически значимая разница по критерию 'SVVI'\n",
      "Статистически значимая разница по критерию 'CVVI'\n"
     ]
    }
   ],
   "source": [
    "found = False\n",
    "for c in criteria:\n",
    "    bests = []\n",
    "    worsts = []\n",
    "    for t in texts.find({'mark': {'$lt': 51}}):\n",
    "        bests.append(t[c])\n",
    "    for t in texts.find({'mark': {'$gt': 74}}):\n",
    "        worsts.append(t[c])\n",
    "    t, p = stats.ttest_ind(bests, worsts)\n",
    "    if 2*p < 0.05:\n",
    "        print(\"Статистически значимая разница по критерию '%s'\" % c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление в базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(path, mark, collection):\n",
    "    with open(path, 'r') as fr:\n",
    "        text = fr.read()\n",
    "    _text = check_spelling(text)\n",
    "    _text = _text.replace('\\n', ' ')\n",
    "    parser = ParserUDpipe(_text)\n",
    "    LC = LexicalComplexity(_text, parser)\n",
    "    info = {'text': text, 'mark': mark, 'path': path[:-4], 'density': LC.density(),\n",
    "            'LS': LC.LS(), 'VSI': LC.VS()[0],\n",
    "            'VSII': LC.VS()[1], 'VSIII': LC.VS()[2], 'LFP_first': LC.LFP()[0], \n",
    "            'LFP_second': LC.LFP()[1], 'LFP_third': LC.LFP()[2], 'LFP_none': LC.LFP()[3], \n",
    "            'NDW': LC.NDW(), 'TTR': LC.TTR()[0], 'CTTR': LC.TTR()[1], 'RTTR': LC.TTR()[2], \n",
    "            'LogTTR': LC.TTR()[3], 'Uber': LC.TTR()[4], 'D': LC.D(), \n",
    "            'LV': LC.LV(), 'VVI': LC.VV()[0], 'SVVI': LC.VV()[1], 'CVVI': LC.VV()[2],\n",
    "            'VVII': LC.VV()[3], 'NV': LC.NV(), 'AdjV': LC.AdjV(), 'AdvV': LC.AdvV(), \n",
    "            'ModV': LC.ModV()}\n",
    "    text_id=collection.insert_one(info).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './exam/exam2017_7/VSa_97_2.txt'\n",
    "mark = 60\n",
    "collection = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert(path, mark, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./exam/exam2017_7/VSa_97_2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./exam/exam2017_7/VSa_97_2.txt'[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление из базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectId('5c8413742964392ea9b5f61c')\n"
     ]
    }
   ],
   "source": [
    "for idx in texts.find({'path': './exam/exam2017_7/VSa_97_2'}):\n",
    "    pprint(idx.get('_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = '5c83ed522964392e04bed230'\n",
    "texts.delete_one( {'_id': id_});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
